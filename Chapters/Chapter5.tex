\externaldocument{chapter1}
\externaldocument{chapter2}
\chapter{Conclusion} % Main chapter title

\label{Chapter5}
The performance of music tagging algorithm will be higher if the extracted features encode the acoustic cues necessary for discriminating the semantics. Supervised feature learning is used to obtain features that could be optimal for our context of music tagging. Convolution and recurrent neural networks were used to induct supervised learning into the feature extraction pipeline. The number of parameters to solve increases as the learning problem becomes deeper and hence more training data is required for convergence. Therefore, in this thesis, transfer learning with models trained on \textit{MagnaTagaTune} dataset were analysed by comparing different levels of fine-tuning on the target dataset. It has been found that fine-tuning entire CNN + RNN on target data boosts the performance and there were significant performance difference with black-box CNN. It would be useful to bridge this gap to improve performance, because deep levels of training on our target data, which is small, struggles to out-perform MFCC features.     

\subsubsection{Proposal for improvements}
\begin{itemize}
\setlength\itemsep{0em}
\item Since MFCC performs better than fine-tuned CNNs, it would be worthy to analyse CNN architectures trained on the source data by initializing CNN filters with cosine functions to get richer features. (In chapter 2, section \ref{stacked}, an illustration of MFCC computation as convolution operations was shown)
\item Singular value decomposition of the converged filters at each CNN layer can be performed to see if they are converging to any known mathematical function. If it does, then it would be useful to set those functions as initialization before training.   
\end{itemize}
 

