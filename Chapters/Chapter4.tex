\externaldocument{chapter1}
\externaldocument{chapter5}
\chapter{Experiments and Results} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use\ref{Chapter2} 

The aim of this thesis is to find the optimal algorithm for content-based multi-label classification of music tracks, that can be solved with minimal training data. We are looking for a classifier that can discriminate aesthetics in music, but the public datasets are socially biased and contains only short excerpts. Hence the multi-label classifiers have to be tested on our unbiased target dataset which has full clips. In Chapter 3, the state of art models were reviewed and in section \ref{model}, the short-comings of these algorithms in addressing the problems discussed in Chapter 1 (see \ref{problems}) were pointed out. In this chapter, the experiments that will lead to finding the best algorithm using the components short-listed in \ref{model} will be described.

\section{Dataset and Evaluation}
\label{dataset}
More specific to our task than representing audio is finding a proper dataset of labelled pairs. To analyse \textit{transfer learning}, a large dataset is needed for the \textit{source task}. Popularly used \textit{Million Song Dataset} (MSD) \cite{MSD} contains a cluster of complimentary datasets, most of them annotated with \textit{social tags}. For instance, \textit{Last.fm} which forms a part of MSD contains annotations from users of an online radio application. But such social tags contribute to the \textit{audio-semantic} noise which we want to eliminate. The dataset that is mostly used for evaluating content-based algorithms is \textit{Magna Tag A Tune} dataset \cite{MTT}, where annotations are gathered through a game application that attracts users who are familiar with technical terms related to music. Hence the tags in this dataset are usually clean. Hence this would be a decent choice for our \textit{source task}.

\subsection{Dataset for source task}
\label{source}
The MagnaTagATune dataset consists of 25,856 clips of 29.1-s mp3 files with 188 tags. These annotations are gathered from an online game called \textit{Tag a Tune}. A player is partnered up with another random player who cannot be communicated. Both listen to some track, and have to select appropriate tags. Then the players are asked one simple question : "Are we listening to same song?". Answering this correctly will earn them points. Frequently matched tags are collected to build a labelled dataset. This dataset is the largest available that comes close to minimizing the \textit{audio-semantic} noise. We use only the top-50 tags for training from this dataset.  

\subsection{Dataset for target task}
\label{target}
To gather annotations with clean mapping to aesthetic properties, we adopt the most straightforward and costly method - ask someone to listen to songs and tag them. Around 900 songs approximately 5 - 8 min long, were tagged by my supervisor Prof. Paolo Bientinesi in association with Prof. Marco Aluno (Professor of Composition and Theory at University EAFIT, Columbia). Out of 900 songs, 100 are used for validation.  

\subsection{Evaluation metrics}
\label{evaluation}
For multi-label classification with L labels, the performance of L binary classifiers is computed and averaged. Each label can belong to one of the class - \textit{positive} (1) or \textit{negative} (0). To discuss about a fair performance measure, lets first recall some important terminologies,\\
\\
\textbf{True Positives (TP) :} If the classifier admits a label as positive when the ground truth is also positive.\\
\textbf{True Negative (TN) :} If the classifier admits a label as negative when the ground truth is also negative.\\
\textbf{False Positives (FP) :} If the classifier admits a label as positive when the ground truth is negative.\\
\textbf{False Negative (FN) :} If the classifier admits a label as negative when the ground truth is positive.\\
\\
For a general tagging problem, most of the tags are \textit{negative} for most of the clips (That is, out of 900 songs, if 20 songs have the tag '\textit{electro}', then for this tag there are 20 \textit{ground truth positives} and 880 \textit{ground truth negatives}.\\
\\
\textbf{Accuracy :} To see why \textit{accuracy} will be an unfair measure, lets look at the definition of \textit{accuracy} for the label 'electro',
\[
  Accuracy = \frac{\sum TP + \sum TN}{900} = \frac{0+880}{900}
\]
Even if the classifier did not classify one 'electro' as positive, accuracy will be 0.98 with the contribution from 880 true negative samples.\\
\\
\textbf{Precision :} This is also not a comprehensive measure to indicate the strength of the classifier because it does not tell anything about the percentage of negatively classified samples (false negatives). That is, even if the classifier \textit{correctly} admits one 'electro' as positive and 899 as negative, \textit{precision} would be 1.0   
\[
  Precission = \frac{\sum TP}{\sum TP + \sum FP} = \frac{1}{1+0}
\]
\\
\textbf{Recall :} This is also not comprehensive because it does not tell anything about false positives or in other words, it does not say if your classifier is a \textit{liar}. That is, even if the classifier admits 900 samples as positive (20 true positives and 880 false positives), \textit{recall} will be 1.0
\[
  Recall = \frac{\sum TP}{\sum TP + \sum FN} = \frac{20}{20+0} 
\]
\\
To strike a balance between \textit{recall} and \textit{precision}, the harmonic mean of both is often used, which is called \textit{F1} score. But to calculate all the metrics mentioned so far, some classifier threshold is required (That is, a binary classifier spits a number between 0 and 1 and if the number is above the threshold, the sample is classified as positive, otherwise negative). If someone changes the threshold then the performance can change. To find a comprehensive measure for classifier performance, the metric should consider all threshold values.\\
\\ 
\textbf{Area under precision-recall curve :} When the\textit{ precision} and \textit{recall} are plotted for various threshold and the area under the precision-recall curve is found, the metric is termed as \textit{average precision}. Averaging the \textit{average precision} of L labels gives \textit{Mean average precision}, which is a useful metric and can be found in some research works.\\
\\
\textbf{Area under receiver operating characteristic curve (AUC) :} The \textit{fall-out} and \textit{recall} are plotted for various threshold and the area under the this curve is found. \textit{Fall-out} is defined as
\[
   Fall\_out = \frac{\sum FP}{\sum FP + \sum TN} 
\]
\textit{Recall} answers the question, 'when the ground truth is positive, how often does the classifier admit it as a positive'. \textit{Fall-out} answers the question, 'when the ground truth is negative, how often does the classifier admit it as positive'. A random classifier would have an AUC of 0.5, meaning, for a binary random classification of an unknown sample there is 50\% chance for it to be true. Therefore, AUC can be thought of as a probability that a classifier would rank a randomly chosen positive ground truth higher than a randomly chosen negative observation. AUC measures how well the positive and negative classes are separated. AUC is computed for L labels and averaged. Since the publications reviewed in previous chapter report this metric, we will also use the same. But in addition, we use \textit{weighted average} because our validation set is small and number of occurrences of each label is not balanced.\\
\\
Therefore, in all our experiments, \textit{Weighted averaged AUC} (WAUC) will be reported.

\section{Experiments}
\label{experiments}
 
As with any MIR task, the raw audio signal containing amplitude values in time domain is first down sampled and representation parameters are fixed (ref. \ref{stft}). This is because computational cost is heavily affected by the size of the input layers.

\subsection{Representation Parameters :}
\label{repPara}
As discussed in section \ref{model} of previous chapter, supervision is not imposed for representation operators. Hence, the following  representation parameters from \cite{choi_cnn} are fixed to compute \textit{log mel power spectrogram} for all experiments. 

\begin{table}[H]
\label{tab:repPara}
\centering
\begin{tabular}{| p{.4\textwidth} | p{.3\textwidth}|}
\hline
\textbf{Parameter} & \textbf{Setting}\\
\hline
Sampling rate & 12 KHz\\
\hline
STFT window function & Hamming window\\
\hline
Size of each segment in STFT & 512 (42 ms)\\
\hline
Hop-Length & 256\\
\hline
FFT Size & 512\\
\hline
Mel bins & 96\\
\hline
\end{tabular}
\caption{Representation Parameters} 
\end{table}

\subsection{Analyses of Convolution Neural Networks}
\label{pretrained}
In Chapter 2 (ref. \ref{stacked}), it was shown how Convolution Neural Networks (CNN) are motivating to be used as feature extractor for music signal. In Chapter 3 (ref. \ref{convolution}) some of the successful mel-spectrogram convolution architectures trained on MTT dataset showing state-of-art performance were discussed. Now we would like to see if such features extracted through these models can be used for auxiliary tasks. That is to say, the learned CNN parameters (weights) from \textit{source} dataset are used as initialization setting for \textit{target} tasks.    
\bigskip

\noindent The CNN architecture from \cite{choi_cnn} achieves the best AUC score on MTT dataset and hence this architecture is used for feature extraction. The algorithm for their model ($CHOI\_CNN$) is explained in section \ref{convolution} (Algorithm \ref{alg:choicnn}). The input to their CNN was 29.1s mel-spectrogram with representation parameters mentioned above (Thus resulting in 1366 time samples). So for our task, features are extracted every 29.1s and sequentially sent to RNN. The temporal summarization is done by 2 layer \textit{Long Short-Term Memory Recurrent Neural Network} (ref. \ref{rnn}). The RNN module does sequence to one mapping ($Seq2One$) of the input features. This entire model is then trained for 150K iteration on MTT dataset with top 50 tags. Their model was already trained on Million Song Dataset \cite{MSD} with top 50 \textit{Last.fm} tags. We just fine-tune their model on MTT dataset after merging clips from same song. Features of clips from same song are sequentially given as input to RNN with a dropout (ref. \ref{training}) of 0.3 after each layer, which then projects to a fixed sized feature vector. The output of RNN is then passed to a fully connected layer with 50 output units and \textit{sigmoid} activation. ADAM optimizer with \textit{binary-cross-entropy} loss function is used for training. The starting learning rate is $0.001$, decaying at $1^{-8}$ and beta $0.99$ (ref. \ref{training}). Algorithm for $L$ labels is described below and the notations used are consistent with formalisms in Chapter \ref{Chapter2}. The algorithm is implemented in \textit{Torch} [torch] and mel-spectrogram was extracted using \textit{Librosa} [librosa]
\bigskip 

\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }
  \label{alg:cnnrnn}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{L}$ 
    \State $\textbf{C} = STFT(\textbf{a})$ \Comment{$\textbf{C} \in \mathbb{C}^{M \times P}$}
    \State $\textbf{Y}_{r} = Log(\textbf{C} \odot \textbf{C})$ \Comment{$\textbf{Y}_{r} \in \mathbb{R}^{M \times P}$}
    \State $\textbf{R} = MEL(\textbf{Y}_{r})$ \Comment{$\textbf{R} \in \mathbb{R}^{96 \times P}$}
    \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{96 \times 1366}$}
      \State $\textbf{Y}[i] \leftarrow CHOI\_CNN(\textbf{X})$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$} 
    \EndFor
    \State $\textbf{Y}_{1} = Drop_{(0.3)}(Seq2Seq\_LSTM(\textbf{Y}))$ \Comment{$\textbf{Y}_{1} \in \mathbb{R}^{1024 \times W}$}
    \State $\textbf{y}_{2} = Drop_{(0.3)}(Seq2One\_LSTM(\textbf{Y}_{1}))$ \Comment{$\textbf{y}_{2} \in \mathbb{R}^{1024}$}
    \State $Pred = \sigma(L(\textbf{W})\textbf{y}_{2}))$ \Comment{$\textbf{W} \in \mathbb{R}^{L \times 1024}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\noindent This CNN model can either be used as a \textit{black-box} feature extractor (That is, weights of the model are not modified while training the \textit{target-task}) or certain layers can be \textit{fine-tuned} (That is, we continue the training on \textit{target-task}). Both the cases are looked separately,
\bigskip

\noindent \textbf{Blackbox CNN + RNN :}\\ 
The weights of CNN trained on the source task are not modified (no fine-tuning). The weights of RNN are also initiazied with those trained on source task. The fully-connected layer in the source task is changed to 65 output units. (i.e 65 labels). The network is trained by back-propagating through the fully connected layer and RNN with \textit{binary-cross entropy} loss function (ref. \ref{training}). The optimization parameters are same as that of \textit{source task}. Training is stopped after 25K iterations, after which the model begins to over-fit. Weighted averaged AUC (WAUC) was \textbf{0.65}
\bigskip

\noindent \textbf{Fine-tune CNN + RNN :}\\
With the same parameter settings, the last layer of CNN was finetuned after 5K iterations. Training was then continued until 25K iterations and WAUC went up to \textbf{0.69}. When last two CNN layers were finetuned WAUC further improved to \textbf{0.71}. Fine-tuning earlier layers proved sub-optimal. 
\bigskip

\noindent This tells us that in the final layers CNN tend to find features specific to task. (Recalling that labels for source and target tasks are different). However, we still do not know if convolutions over mel-spectrogram will be better than MFCCs which is proven to model audio discriminants. Before going there, the effectiveness of RNN should also be questioned. It was hypothesised in Chapter 3 (\ref{model}) that Bag-of-Frames (\ref{clustering}) features using K-Means (which was actually used in \cite{MultiScale} to attain state of art performance) may not be suitable for summarizing features for longer audio. So we test this by replacing RNN with Bag Of Frames (BoF) features.
\bigskip

\noindent \textbf{CNN + BoF :}
The CNN is first finetuned for 10K iterations with algorithm \ref{alg:cnnrnn}. Then 1024 centroids of CNN features are found by unsupervised training on both MTT and our target dataset. This is followed by multi layer perceptron with a hidden layer of 512 units and ReLU activation. Having hidden size of 1024 did not improve the result. WAUC was \textbf{0.67}. The algorithm is described below,

\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:cnnbow}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{C} = STFT(\textbf{a})$ \Comment{$\textbf{C} \in \mathbb{C}^{M \times P}$}
    \State $\textbf{Y}_{r} = Log(\textbf{C} \odot \textbf{C})$ \Comment{$\textbf{Y}_{r} \in \mathbb{R}^{M \times P}$}
    \State $\textbf{R} = MEL(\textbf{Y}_{r})$ \Comment{$\textbf{R} \in \mathbb{R}^{96 \times P}$}
    \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{96 \times 1366}$}
      \State $\textbf{Y}[i] \leftarrow CHOI\_CNN(\textbf{X})$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$} 
    \EndFor
    \State $\textbf{y}_{1} = BagOfFrames(\textbf{Y},1024)$ \Comment{$\textbf{y}_{1} \in \mathbb{R}^{1024}$}
     \State $Pred = \sigma(L(\textbf{W}_{2})ReLU(L(\textbf{W}_{1})\textbf{y}_{1}))$ \Comment{$\textbf{W}_{2} \in \mathbb{R}^{65 \times 512}, \textbf{W}_{1} \in \mathbb{R}^{512 \times 1024}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier
    
\subsection{Comparision of MFCC with CNN features}    
\label{mfcc}  
MFCCs are still de-facto standard for classifications on small datasets. If CNNs had to outperform MFCCs, the learned parameters should have to encode discriminants similar to MFCCs. MFCCs are computed by taking discrete-cosine transform on log mel-spectrogram (ref. \ref{basis}). Following the comparison strategies from \cite{choi_cnn}, we retain 30 coefficients, their first and second derivative, resulting in a vector of size 90 for each STFT frame.
\bigskip

\noindent \textbf{MFCC + RNN :}\\
Now, MFCCs from every STFT window is passed in a \textit{30s  Batched sequence} to a Sequence to one LSTM, which results in a projection for every 30s window. These sequence of 30s frames are then passed to another Sequence to One LSTM to get a final projection. This is done because a MFCCs from STFT frame will result in a long sequence and RNNs tend to forget the information in the earlier sequence samples. This network was first trained with MTT dataset before our target dataset. The parameter setting are same as those used while training \textbf{CNN+RNN}. The resulting WAUC was \textbf{0.74}   
\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:mfccrnn}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{R} = MFCC(\textbf{a})$ \Comment{$\textbf{R} \in \mathbb{R}^{90 \times P}$}
        \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{90 \times 1366}$}
    \State $\textbf{Y}[i] \leftarrow Drop_{(0.3)}(Seq2One\_LSTM(\textbf{X}))$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$}
    \EndFor
    \State $\textbf{y} = Drop_{(0.3)}(Seq2One\_LSTM(\textbf{Y}))$ \Comment{$\textbf{y} \in \mathbb{R}^{1024}$}
    \State $Pred = \sigma(L(\textbf{W})\textbf{y}))$ \Comment{$\textbf{W} \in \mathbb{R}^{65 \times 1024}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\noindent \textbf{MFCC + BoF :}\\
RNN is repalaced with Bag of Frames features. Now WAUC drops to \textbf{0.62}
\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:mfccbow}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{R} = MFCC(\textbf{a})$ \Comment{$\textbf{R} \in \mathbb{R}^{90 \times P}$}
   \State $\textbf{y} = BagOfFrames(\textbf{R},1024)$ \Comment{$\textbf{y} \in \mathbb{R}^{1024}$}
     \State $Pred = \sigma(L(\textbf{W}_{2})ReLU(L(\textbf{W}_{1})\textbf{y}))$ \Comment{$\textbf{W}_{2} \in \mathbb{R}^{65 \times 512}, \textbf{W}_{1} \in \mathbb{R}^{512 \times 1024}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier    

\section{Summary of Results}
\label{results}
Summary of results is shown in the table below. It is seen that \textit{transfer learning} of convolutions over mel-spectrogram with architecture in \cite{choi_cnn}  cannot match with MFCCs for small datasets. This indicates that convolutional features from source dataset are more task-specific. It is also seen that \textit{Recurrent Neural Networks} perform better in summarizing features for longer audio. This indicates the existence of rhythmic patterns that discriminate music. 

   \begin{tabular}{ | p{5cm} | l |}
    \hline
    \textbf{Model} & \textbf{AUC} \\ \hline
    Finetune CNN + RNN &  0.71\\ \hline
    CNN + BoF  &  0.67\\ \hline
    MFCC + RNN &  \textbf{0.74} \\ \hline
    MFCC + BoF &  0.62 \\ \hline
    \hline
    \end{tabular}

