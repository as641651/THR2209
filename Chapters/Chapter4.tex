\externaldocument{chapter1}
\chapter{Experiments and Results} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use\ref{Chapter2} 

The aim of this thesis is to find the optimal algorithm for content-based multi-label classification of music tracks, that can be solved with minimal training data. We are looking for a classifier that can discriminate aesthetics in music, but the public datasets are socially biased and contains only short excerpts. Hence the multi-label classifiers have to be tested on our unbiased target dataset which has full clips. In Chapter 3, the state of art models were reviewed and in section \ref{model}, the short-comings of these algorithms in addressing the problems discussed in Chapter 1 (see \ref{problems}) were pointed out. In this chapter, the experiments that will lead to finding the best algorithm using the components short-listed in \ref{model} will be described.

\section{Dataset and Evaluation}
\label{dataset}
More specific to our task than representing audio is finding a proper dataset of labelled pairs. Recalling that our aim is to identify the aesthetic properties of music from the audio content, a dataset that is free from \textit{audio-semantic} noise is needed. Furthermore, to test \textit{transfer learning}, a large dataset is needed for the \textit{source task} (ref. \ref{transfer}). Popularly used \textit{Million Song Dataset} (MSD) \cite{MSD} contains a cluster of complimentary datasets, most of them annotated with \textit{social tags}. For instance, \textit{Last.fm} which forms a part of MSD contains annotations from users of an online radio application. But such social tags contribute to the \textit{audio-semantic} noise which we want to eliminate. The dataset that is mostly used for evaluating content-based algorithms is \textit{Magna Tag A Tune} dataset \cite{MTT}, where annotations are gathered through a game application that attracts users who are familiar with technical terms related to music. Hence the tags in this dataset are usually clean. Hence this would be a decent choice for our \textit{source task}.

\subsection{Dataset for source task}
\label{source}
The MagnaTagATune dataset consists of 25,856 clips of 29.1-s, 16 kHz-sampled mp3 files with 188 tags. These annotations are gathered from an online game called \textit{Tag a Tune}. A player is partnered up with another random player who cannot be communicated. Both listen to some track, and have to select appropriate tags. Then the players are asked one simple question : "Are we listening to same song?". Answering this correctly will earn them points. This dataset is the largest available that comes close to minimizing the \textit{audio-semantic} noise. However, social factor still plays a role here. This is partly indicated by tag frequency where the most frequent tag is used 4,851 times while the $50^{th}$ most frequent one used 490 times in the training set. We use only the top-50 tags for training from this dataset.  

\subsection{Dataset for target task}
\label{target}
To gather annotations with clean mapping to aesthetic properties, we adopt the most straightforward and costly method - ask someone to listen to songs and tag them. Around 900 songs approximately 5 - 8 min long, were tagged by my supervisor Prof. Paolo Bientinesi in association with Prof. Marco Aluno (Professor of Composition and Theory at University EAFIT, Columbia). Out of 900 songs, 112 are used for validation.  

\subsection{Evaluation metrics}
\label{evaluation}

\section{Experiments}
\label{experiments}
As with any MIR task, the raw audio signal containing amplitude values in time domain is first down sampled and representation parameters are fixed (ref. \ref{stft}). This is because computational cost is heavily affected by the size of the input layers.
\bigskip
 
\noindent \textbf{Sampling Parameters : } Although most of the available audio in digital format are sampled at 44KHz (ref. \ref{sampling}), it is important to note that most of the information lie in the lower range of the spectrum. In \cite{choi_cnn}, a pilot experiment was conducted to demonstrate similar performance with 12KHz and 16KHz for top 50 tags (Recall that MTT clips are already downsampled to 16KHz). Hence we sample all our tracks to 12KHz. 
\bigskip

\noindent Next step is to extract relevant features. General pipeline is \textit{sampling} (ref. \ref{sampling}), \textit{representation} (ref. \ref{stft}), stacks of \textit{dimensionality reduction} (ref. \ref{dimension}) followed by \textit{temporal summarization} (ref. \ref{temporal}). Feature learning can be introduced at different stages. Introducing in earlier stages would require training with huge amount of data. Feature learning on raw sampled signal proved sub-optimal in [end to end]. Same was the case when convolved over STFT frame \cite{choi_cnn}. Convolutions over mel-spectrogram performed better than MFCC in many previous work \cite{choi_cnn}[end to end]. Hence we stick to engineered features until the extraction of mel-spectrogram.  
\bigskip 

\noindent \textbf{Mel-Spectrogram Parameters :}
The signal in the time domain is converted to frequency domain by Short-Time-Fourier-Transform ($STFT$) using Fast-Fourier-Transform(FFT) algorithm. The arguments for doing this were presented in Chapter 2 (ref. \ref{stft}). The parameters for FFT are the size (often referred as FFT Size) and stride (often referred as hop-length) of the window function. FFT size was fixed to 512 (42 ms) and hop-length was fixed to 256. Motivated by the human auditory system, the frequency axis is binned to mel-scale and log of squared STFT coefficients (proportional to loudness) are calculated. In \cite{choi_cnn}, it was stated that 96 mel-bins were optimum.   
\bigskip

\noindent Feature learning over mel-spectrogram still requires large dataset, but our target dataset is small. Hence by questing the effectiveness of feature learning over spectrogram with MFCCs, we decided  to compare both.
\bigskip

\subsection{Experiments with pre-trained CNNs as feature extractor}
\label{pretrained}
In Chapter 2 (ref. \ref{stacked}), it was shown how Convolution Neural Networks (CNN) are motivating to be used as feature extractor for music signal. In Chapter 3 (ref. \ref{convolution}) some of the successful mel-spectrogram convolution architectures trained on MTT dataset showing state-of-art performance were discussed. Now we would like to see if such features extracted through these models can be used for auxiliary tasks. That is to say, the learned CNN parameters (weights) from \textit{source} dataset are used as initialization setting for \textit{target} tasks.    
\bigskip

\noindent The CNN architecture from \cite{choi_cnn} achieves the best AUC score on MTT dataset and hence this architecture is used for feature extraction. The algorithm for their model ($CHOI\_CNN$) is explained in section \ref{convolution} (Algorithm \ref{alg:choicnn}). The input to their CNN was 29.1s mel-spectrogram with representation parameters mentioned above (Thus resulting in 1366 time samples). So for our task, features are extracted every 29.1s and sequentially sent to RNN. The temporal summarization is done by 2 layer \textit{Long Short-Term Memory Recurrent Neural Network} (ref. \ref{rnn}). The RNN module does sequence to one mapping ($Seq2One$) of the input features. This entire model is then trained for 150K iteration on MTT dataset with top 50 tags. Their model was already trained on Million Song Dataset \cite{MSD} with top 50 \textit{Last.fm} tags. We just fine-tune their model on MTT dataset after merging clips from same song. Features of clips from same song are sequentially given as input to RNN with a dropout (ref. \ref{training}) of 0.3 after each layer, which then projects to a fixed sized feature vector. The output of RNN is then passed to a fully connected layer with 50 output units and \textit{sigmoid} activation. ADAM optimizer with \textit{binary-cross-entropy} loss function is used for training. The starting learning rate is $0.001$, decaying at $1^{-8}$ and beta $0.99$ (ref. \ref{training}). Algorithm for $L$ labels is described below and the notations used are consistent with formalisms in Chapter \ref{Chapter2}. The algorithm is implemented in \textit{Torch} [torch] and mel-spectrogram was extracted using \textit{Librosa} [librosa]
\bigskip 

\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }
  \label{alg:cnnrnn}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{L}$ 
    \State $\textbf{C} = STFT(\textbf{a})$ \Comment{$\textbf{C} \in \mathbb{C}^{M \times P}$}
    \State $\textbf{Y}_{r} = Log(\textbf{C} \odot \textbf{C})$ \Comment{$\textbf{Y}_{r} \in \mathbb{R}^{M \times P}$}
    \State $\textbf{R} = MEL(\textbf{Y}_{r})$ \Comment{$\textbf{R} \in \mathbb{R}^{96 \times P}$}
    \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{96 \times 1366}$}
      \State $\textbf{Y}[i] \leftarrow CHOI\_CNN(\textbf{X})$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$} 
    \EndFor
    \State $\textbf{Y}_{1} = Drop_{(0.3)}(Seq2Seq\_LSTM(\textbf{Y}))$ \Comment{$\textbf{Y}_{1} \in \mathbb{R}^{1024 \times W}$}
    \State $\textbf{y}_{2} = Drop_{(0.3)}(Seq2One\_LSTM(\textbf{Y}_{1}))$ \Comment{$\textbf{y}_{2} \in \mathbb{R}^{1024}$}
    \State $Pred = \sigma(L(\textbf{W})\textbf{y}_{2}))$ \Comment{$\textbf{W} \in \mathbb{R}^{L \times 1024}$}
  \end{algorithmic}
\end{algorithm}

\noindent This CNN model can either be used as a \textit{black-box} feature extractor (That is, weights of the model are not modified while training the \textit{target-task}) or certain layers can be \textit{fine-tuned} (That is, we continue the training on \textit{target-task}). Both the cases are looked separately,
\bigskip

\noindent \textbf{Blackbox CNN + RNN :}\\ 
The weights of CNN trained on the source task are not modified (no fine-tuning). The weights of RNN are also initiazied with those trained on source task. The fully-connected layer in the source task is changed to 65 output units. (i.e 65 labels). The network is trained by back-propagating through the fully connected layer and RNN with \textit{binary-cross entropy} loss function (ref. \ref{training}). The optimization parameters are same as that of \textit{source task}. Training is stopped after 25K iterations, after which the model begins to over-fit. Weighted averaged AUC (WAUC) was \textbf{0.65}
\bigskip

\noindent \textbf{Fine-tune CNN + RNN :}\\
With the same parameter settings, the last layer of CNN was finetuned after 5K iterations. Training was then continued until 25K iterations and WAUC went up to \textbf{0.69}. When last two CNN layers were finetuned WAUC further improved to \textbf{0.71}. Fine-tuning earlier layers proved sub-optimal. 
\bigskip

\noindent This tells us that in the final layers CNN tend to find features specific to task. (Recalling that labels for source and target tasks are different). However, we still do not know if convolutions over mel-spectrogram will be better than MFCCs which is proven to model audio discriminants. Before going there, the effectiveness of RNN should also be questioned. It was hypothesised in Chapter 3 (\ref{model}) that Bog-of-Words (\ref{clustering}) features using K-Means (which was actually used in [multi-scale] to attain state of art performance) may not be suitable for summarizing features for longer audio. So we test this by replacing RNN with Bag Of Words (BoW) features.
\bigskip

\noindent \textbf{CNN + BoW :}
The CNN is first finetuned for 10K iterations with algorithm \ref{alg:cnnrnn}. Then 1024 centroids of CNN features are found by unsupervised training on both MTT and our target dataset. This is followed by multi layer perceptron with a hidden layer of 512 units and ReLU activation. Having hidden size of 1024 did not improve the result. WAUC was \textbf{0.67}. The algorithm is described below,

\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:cnnbow}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{C} = STFT(\textbf{a})$ \Comment{$\textbf{C} \in \mathbb{C}^{M \times P}$}
    \State $\textbf{Y}_{r} = Log(\textbf{C} \odot \textbf{C})$ \Comment{$\textbf{Y}_{r} \in \mathbb{R}^{M \times P}$}
    \State $\textbf{R} = MEL(\textbf{Y}_{r})$ \Comment{$\textbf{R} \in \mathbb{R}^{96 \times P}$}
    \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{96 \times 1366}$}
      \State $\textbf{Y}[i] \leftarrow CHOI\_CNN(\textbf{X})$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$} 
    \EndFor
    \State $\textbf{y}_{1} = BagOfWords(\textbf{Y},1024)$ \Comment{$\textbf{y}_{1} \in \mathbb{R}^{1024}$}
     \State $Pred = \sigma(L(\textbf{W}_{2})ReLU(L(\textbf{W}_{1})\textbf{y}_{1}))$ \Comment{$\textbf{W}_{2} \in \mathbb{R}^{65 \times 512}, \textbf{W}_{1} \in \mathbb{R}^{512 \times 1024}$}
  \end{algorithmic}
\end{algorithm}
    
\subsection{Experiments with MFCCs as feature extractor}    
\label{mfcc}  
MFCCs are still de-facto standard for classifications on small datasets. If CNNs had to outperform MFCCs, the learned parameters should have to encode discriminants similar to MFCCs. MFCCs are computed by taking discrete-cosine transform on log mel-spectrogram (ref. \ref{basis}). Following the comparison strategies from \cite{choi_cnn}, we retain 30 coefficients, their first and second derivative, resulting in a vector of size 90 for each STFT frame.
\bigskip

\noindent \textbf{MFCC + RNN :}\\
Now, MFCCs from every STFT window is passed in a \textit{30s  Batched sequence} to a Sequence to one LSTM, which results in a projection for every 30s window. These sequence of 30s frames are then passed to another Sequence to One LSTM to get a final projection. This is done because a MFCCs from STFT frame will result in a long sequence and RNNs tend to forget the information in the earlier sequence samples. This network was first trained with MTT dataset before our target dataset. The parameter setting are same as those used while training \textbf{CNN+RNN}. The resulting WAUC was \textbf{0.74}   
\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:mfccrnn}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{R} = MFCC(\textbf{a})$ \Comment{$\textbf{R} \in \mathbb{R}^{90 \times P}$}
        \State $W = floor(\frac{P}{1366})$
    \For{$i \in \{0,..,W\}$}
      \State $\textbf{X} \leftarrow \textbf{R}[:][i:(i+1).1366]$ \Comment{$\textbf{X} \in \mathbb{R}^{90 \times 1366}$}
    \State $\textbf{Y}[i] \leftarrow Drop_{(0.3)}(Seq2One\_LSTM(\textbf{X}))$ \Comment{$\textbf{Y} \in \mathbb{R}^{1024 \times W}$}
    \EndFor
    \State $\textbf{y} = Drop_{(0.3)}(Seq2One\_LSTM(\textbf{Y}))$ \Comment{$\textbf{y} \in \mathbb{R}^{1024}$}
    \State $Pred = \sigma(L(\textbf{W})\textbf{y}))$ \Comment{$\textbf{W} \in \mathbb{R}^{65 \times 1024}$}
  \end{algorithmic}
\end{algorithm}

\noindent \textbf{MFCC + BoW :}\\
RNN is repalaced with Bag of Words features. Now WAUC drops to \textbf{0.62}
\begin{algorithm}
  \caption{$Pred$ = MODEL($\textbf{a}$) }\label{alg:mfccbow}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $Pred \in \mathbb{R}^{65}$ 
    \State $\textbf{R} = MFCC(\textbf{a})$ \Comment{$\textbf{R} \in \mathbb{R}^{90 \times P}$}
   \State $\textbf{y} = BagOfWords(\textbf{R},1024)$ \Comment{$\textbf{y} \in \mathbb{R}^{1024}$}
     \State $Pred = \sigma(L(\textbf{W}_{2})ReLU(L(\textbf{W}_{1})\textbf{y}))$ \Comment{$\textbf{W}_{2} \in \mathbb{R}^{65 \times 512}, \textbf{W}_{1} \in \mathbb{R}^{512 \times 1024}$}
  \end{algorithmic}
\end{algorithm}
    

\section{Summary of Results}
\label{results}
Summary of results is shown in the table below. It is seen that \textit{transfer learning} of convolutions over mel-spectrogram with architecture in \cite{choi_cnn}  cannot match with MFCCs for small datasets. This indicates that convolutional features from source dataset are more task-specific. It is also seen that \textit{Recurrent Neural Networks} perform better in summarizing features for longer audio. This indicates the existence of rhythmic patterns that discriminate music. 

   \begin{tabular}{ | p{5cm} | l |}
    \hline
    \textbf{Model} & \textbf{AUC} \\ \hline
    Finetune CNN + RNN &  0.71\\ \hline
    CNN + BoW  &  0.67\\ \hline
    MFCC + RNN &  \textbf{0.74} \\ \hline
    MFCC + BoW &  0.62 \\ \hline
    \hline
    \end{tabular}

