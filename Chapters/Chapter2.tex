% Chapter 2

\externaldocument{chapter3}
\externaldocument{chapter4}
\externaldocument{chapter5}
\externaldocument{chapter1}
\chapter{Formalisms} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 


%----------------------------------------------------------------------------------------

%\section{Welcome and Thank You}
%In this chapter, acoustical characteristics of music signal that enables general MIR tasks will be introduced.We will examine the Fourier Series representations of sound waves and see how they relate to harmonics and tonal color of instruments  



%----------------------------------------------------------------------------------------

\section{Representation of music signal}

The general aim of Music Information Retrieval (MIR) is to extract information about it's discriminants from the observed data. This observed signal is traditionally represented in the time domain. The time domain is a record of what happened to a parameter of the system versus time. Standard formats use amplitude versus time. The observed signal is then discretised by sampling and stored in digital format (see \ref{sampling}). This signal in the time domain is then changed to frequency domain. This is simply because our ear-brain combination is an excellent frequency domain analyser. Our brain splits the audio spectrum into many narrow bands and determines the power present in each band. Conversion from time domain to frequency domain is done using the foundations from \textit{Fourier theorem} (see \ref{time}). Currently used music signal representations for general MIR tasks are explained in section \ref{stft}.


\subsection{Sampling of continuous-time signal}
\label{sampling}
The digital formats contain the discrete version of the signal obtained by sampling continuous-time signal. Let $s(t)$ be the continuous signal to be sampled, and let sampling be performed by measuring the value of the continuous signal every T seconds, which is called the sampling interval or the sampling period. The sampling frequency or sampling rate, $f_{s}$, is the average number of samples obtained in one second (samples per second),  
\[
 f_{s} = \frac{1}{T}.
\]
The optimum sampling rate is given by Nyquist-Shannon sampling theorem which says, the sampling frequency $f_{s}$ should be at least twice the highest frequency contained in the signal. Given the human hearing range lies between 20Hz - 20KHz, most of the digital audio formats use a standard sampling frequency of 44.4Khz. The signal is further down sampled depending on the kind of feature information needed for classification. 

\subsection{Time-Frequency transformations}
\label{time}
The digital signal is represented in the time domain with amplitude values at each time $t$. This representation has to be changed to frequency domain. Mapping from time-domain to frequency-domain is looked up on as basis transformation. 

\subsubsection{Basis transformation from time to frequency domain}
\noindent The signal in the time domain $\textbf{a}$ is a set of ordered \textit{n}-tuples of real numbers \( (a_{1},a_{2}, ...,a_{N}) \in \mathbb{R}^N \) in the vector space \textit{V} , specifically \textit{Euclidean n-space}. That is to say, a discrete-time signal can be represented as a linear combination of Cartesian basis vectors. The coefficients in linear combination are then the co-ordinates of the corresponding basis system.  
\[
\textbf{a} = (a_{1},a_{2}, ...,a_{N}) = a_{1}\textbf{e}_{1} + a_{2}\textbf{e}_{2} + ... + a_{N}\textbf{e}_{N} = \displaystyle\sum_{t=1}^{N}a_{t}\textbf{e}_{t} = \mathbb{I}\textbf{a}
\]
Where $(a_{1},a_{2}, ...,a_{N})$ are the co-ordinates of Cartesian basis formed by basis vectors $\textbf{e}_{1} ... \textbf{e}_{N}$ with $t \in \{1,2,..N\}$. The unit vector $\textbf{e}_{t} \in \mathbb{R}^{N}$ has 1 in the $t^{th}$ index and 0 everywhere else.
\bigskip

\noindent To transform to frequency domain, we need to find a set of basis vectors $\bm{\phi}_{ \omega }$ that are functions of frequencies ($\omega$). Then the co-ordinates of this basis system $c_{ \omega }$ represents the signal in frequency domain. 
\begin{equation}
\label{exp_fourier}
\textbf{a} = \displaystyle\sum_{ \omega =1}^{M}c_{ \omega }\bm{\phi}_{ \omega } = \bm{\Phi}\textbf{c} \qquad \bm{\Phi} \in \mathbb{C}^{N \times M}, \textbf{c} \in \mathbb{C}^{M}
\end{equation}
If $\bm{\Phi}$ is known, then the transformed co-ordinates $\textbf{c} = (c_{1},c_{2},..,c_{M})$ can be computed as,
\[
\textbf{c} = \bm{\Phi}^{-1}\textbf{a}
\]
$\textbf{c}$ is the transformed representation. $\bm{\Phi}^{-1}$ is the operator that transforms the signal. 

\subsubsection{Exponential Fourier Series and Fourier Transform}
From the definition of \textit{exponential Fourier Series}, any function \textit{periodic} in $\{1,2,...T\}$ can be expanded with series of complex exponentials\cite{allen}. These complex exponentials which are functions of harmonically related frequencies($k \omega$) form basis 
\[
\bm{\phi}_{k}(t) = \frac{1}{\sqrt{T}}e^{ik \omega t} \qquad t \in \{1,2,..T\}
\] 
It is difficult to assume periodicity for a generalized signal. Hence, the Fourier series can not be applied directly and Fourier Transform was developed. By Fourier transform, quantity of each frequency component $\omega$ in an arbitrary signal $\textbf{a}(t)$ can be computed by dividing by $e^{i \omega t}$. Application of Fourier transform to a discrete signal is called \textit{Discrete Fourier Transform}
\[
c_{\omega} =  \displaystyle\sum_{t=1}^{N}a_{t}e^{-i \omega t} \qquad t \in \{1,2,...,N\}
\] 
Thus the transformation operator 
\[
\bm{\Phi}^{-1}[ \omega ] = e^{-i \omega \textbf{t}} \qquad \textbf{t} \in \mathbb{R}^{N}, \omega \in \{1,2,..,M\}
\]
Fast Fourier Transform(FFT) is an efficient implementation of Discrete Fourier Transform(DFT) which exploits the symmetry of $sines$ and $cosines$ in the complex exponential. While DFT requires $O(N^2)$ operations, FFT requires only $O(NlogN)$ \cite{allen}. 


\subsection{STFT, Mel-Spectrogram}
\label{stft}
It is useful to perform FFT locally over short segments. This is because we are more interested in the evolution of frequency content rather than the frequency content of the entire signal. Hence, the full length signal is divided into short segments, and FFT is computed separately for each segment. This is known as \textbf{Short Time Fourier Transform (STFT)}. One common problem with STFT is the \textit{spectral leakage}, which is addressed by modifying the original signal with some window function. The most common window function is the \textbf{Hamming Window} defined as,
\begin{equation}
h[n] = 0.54 - 0.46cos(\frac{2 \pi n}{N-1})
\end{equation}
Where $n \in {1,2,...,F}$ is index of the window function of size $F$ .The signal approaches zero near $n=1$ and $n=N$, but reaches peak near $n=N/2$ \cite{specLeak}. To overcome the information loss at the ends of the window, signal is divided into segments that are partly \textit{overlapping} with each other. The distance between the start of two adjacent segments is called \textit{hop-length}. Figure \ref{fig:stftPipe} shows the extraction of spectral frames of a spectrogram. Thus, the parameters of STFT include 
\begin{itemize}
    \setlength\itemsep{0em}
    \item Choice of window function
    \item Size of each segment in $\textbf{a}$ ($F$)
    \item Hop length or stride ($s$) of the transformation operator
    \item Size of frequency dimension $M$ (also known as FFT size)
\end{itemize}

\begin{figure}[h]
       \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{stft_pipe}
        \caption{Windowing is applied on overlapping segments\\ followed by FFT }
        \label{fig:stftPipe}
       \end{subfigure}
	    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{stft}
        \caption{
        Application of Hamming Window on \\a segment of input signal
        }
        \label{fig:HammingWindow}
       \end{subfigure}
       \caption{(a) Shows STFT Pipeline. (b) Shows the application of \\Window function}\label{fig:STFT}
\end{figure}
\FloatBarrier

\subsubsection{STFT as Convolution}
\noindent The strided operation over the signal $\textbf{a}$ is mathematically represented as a \textbf{convolution}. The resulting spectrogram has $P$ frames. The discrete STFT (\textit{slow}) for $p^{th}$ frame of signal $\textbf{a}$ is obtained as,
\begin{equation}
\label{stfteq}
\textbf{C}[p, \omega ] = \displaystyle\sum_{n=p.s}^{p.s + F}\textbf{a}[n] \Big( \textbf{h}[n-p.s] . e^{-i \omega (n-p.s)} \Big)
\end{equation}
Where:
\begin{itemize}[label=]
    \setlength\itemsep{0em}
    \item $P$: is the number of spectral frames; $p \in [1,2...,P]$ 
    \item $M$: is the dimension of discrete frequency space ; $\omega \in \mathbb{R}^{M}$
    \item $F$: is the frame length 
    \item $s$: is stride (or) hop-length for the next segment
    \item $\textbf{a} \in  \mathbb{R}^{N}$ ; $n \in [1,2,...,N]$
    \item $\textbf{h} \in  \mathbb{R}^{F}$
    \item $\omega \in  \mathbb{R}^{M}$
    \item $\textbf{C} \in \mathbb{C}^{M \times P}$
\end{itemize}
\noindent Equation \ref{stfteq} can be seen as a \textbf{discrete convolution} over the signal \textbf{a} with the operator $\textbf{W}_{STFT}$ which has finite support over the set $\{1..,F\}$ with stride $s$
\[
\textbf{C} = \textbf{a} \star \big( \textbf{h} \bm{\Phi}^{-1} \big)
\]
\begin{equation}
\label{eq:stft}
\textbf{C} = \textbf{a} \star {\textbf{W}^{(s)}}_{STFT}
\end{equation}
where $\textbf{W}_{STFT} = \textbf{h} \bm{\Phi}^{-1}$ is the STFT operator that transforms the signal $\textbf{a}$ from time to frequency domain. 
\bigskip

\subsubsection{Power spectrogram}
\noindent It is important to note that the coefficient matrix $\textbf{C}$ may be complex valued. To obtain useful metrics, we need to extract some physical quantity from the coefficients. This is where \textbf{Parseval's theorem} is used, which relates and time and frequency domain components in DFT as follows \cite{allen} :
\begin{equation}
{\|\textbf{c}\|}^2 \propto {\|\textbf{a}\|}^2
\end{equation}
If \textbf{a} represents amplitude in the time-domain, then as a consequence of Hook's law  on energy equation, we know that
\begin{equation}
Energy \propto amplitude^2
\end{equation}
Thus, it can be inferred that \textbf{square} of the Fourier coefficients is proportional to the energy distributed in the corresponding frequencies. This spectrogram with squared coefficients is called the \textbf{Power Spectrum (P)}. It is often motivating to use this representation because \textit{loudness} is proportional to \textit{energy}.
\begin{equation}
\label{energy}
\textbf{P} = \textbf{C} \odot \textbf{C}
\end{equation}

\noindent The frequencies in the considered range are  grouped into bins. It is useful to do so, due to the aliasing effect of human auditory system. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds.
  
\subsubsection{Mel Power Spectrogram}
\label{mel}

The \textit{mel-scale} was developed to express measured frequency in terms of psychological metrics (i.e perceived pitch). The mel-scale was developed
by experimenting with the human ears interpretation of a pitch. The experiment showed that the pitch is linearly perceived in the frequency range 0-1000 Hz. Above
1000 Hz, the scale becomes logarithmic. There are several formulae to convert Hertz to mel. The following formula is used in this thesis\cite{speech}
\begin{equation}
\omega_{m} = 2595log_{10}\bigg(1+\frac{ \omega }{700}\bigg)
\end{equation}
Where $\omega$ is the frequency in Hertz. In a mel spectrogram, the frequencies and converted to mels and then grouped into mel-spaced bins. This is done by multiplying the spectrum with some \textbf{filter bank ($\textbf{W}_{MEL}$)}. For details about computation of mel-filter banks, refer \cite{mel}. Each filter bank is centered at a specific frequency. Hence, to compute R mel bins, we need R mel-filter banks. The resulting mel power spectrogram ($\textbf{X}$) is
\begin{equation}
\label{mel_conv}
\textbf{X} = \textbf{W}_{MEL}\textbf{P}
\end{equation}
\[
 \textbf{W}_{MEL} \in  \mathbb{R}^{R \times M}, \textbf{P} \in \mathbb{R}^{M \times P}, \textbf{X} \in \mathbb{R}^{R \times P}
\]

\noindent The above Matrix-Matrix multiplication can be represented as a convolution with window size and stride equal to $M$. We can re-write equation \ref{mel_conv} as, 
\begin{equation}
\label{mel_conv_flat}
\textbf{X}[p,\omega_{m}] = \displaystyle\sum_{k=p.M}^{p.M + K}\textbf{p}[k]\textbf{W}_{MEL}(k-p.M)
\end{equation}
Where:
\begin{itemize}[label=]
    \setlength\itemsep{0em}
    \item $\textbf{p}[k]$ = $\textbf{P}[i,j]$ ; $i = floor(\frac{k}{M})$ ; $j = k-floor(\frac{Mk}{M-1})$
    \item $\omega = k-p.M \in \mathbb{R}^{M}$
    \item $\textbf{X} \in \mathbb{R}^{M \times P}$
    \item $\textbf{p} \in \mathbb{R}^{M.P}$
    \item $\omega_{m} \in  \mathbb{R}^{R}$
\end{itemize}

\noindent Hence, we can represent mel-power spectrogram as \textbf{M-strided convolution} and stride $M$ over \textit{flattened} $\textbf{P}$ with mel filters $\textbf{W}_{MEL}$ (i.e, the frequency axis of $\textbf{P}$ is contracted with each mel-filter, 
\begin{equation}
\label{eq:mel}
\boxed
{
  \textbf{X} = \textbf{P} \star \textbf{W}_{MEL}
}
\end{equation}  
Thus the extraction of mel-power spectrogram can be summarized in the following algorithm 
\begin{algorithm}
  \caption{$\textbf{X}$ = $R_{(MEL)}$($\textbf{a}$)}\label{alg:mel}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $\textbf{X} \in \mathbb{R}^{R \times P}$
	\State $\textbf{C} = \textbf{a} \star \textbf{W}_{STFT}$
	\State $\textbf{P} = \textbf{C} \odot \textbf{C}$
	\State $\textbf{X} = \textbf{P} \star \textbf{W}_{MEL}$
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\section{Dimensionality Reduction}
\label{dimension}
The objective of dimensionality reduction is to retain only the information that contribute to discrimination and discard the rest. This is done because the \textit{representation} ($\textbf{X}$) can be large for longer audio tracks (because number of frames $P$ depends on length of the audio). In this thesis, only \textit{mel-spectrogram representation} is considered. The dimensionality reduction of input signal $\textbf{a}$ is generalized as follows,
\[
   \textbf{X} = R(\textbf{a}) \qquad R : \mathbb{R}^{N} \rightarrow \mathbb{R}^{R \times P}
\]
\begin{equation}
\label{dim_red_abstract}
   \textbf{Y} = D(\textbf{X}) \qquad D : \mathbb{R}^{R \times P} \rightarrow \mathbb{R}^{T \times W} 
\end{equation}
 
\noindent The computation of mel-spectrogram defined in the previous section can be a part of the function $R$. The dimensionality reduction is defined by function $D$. The output of reduction $\textbf{Y} \in \mathbb{R}^{T \times W}$ will be the reduced representation ($T < R$ or $W < P$). Depending on how the function $D$ is defined, the following three methods will be discussed
\begin{itemize}
  \item \textbf{Principal Component Analysis} (PCA) : Reduction by \textit{unsupervised learning}.
  \item \textbf{Mel-Frequency Cepstrum Coefficients} (MFCC) : Reduction by domain engineering.
  \item \textbf{Convolution Neural Networks} (CNN) : Reduction by \textit{supervised learning}
\end{itemize} 
\begin{figure}[h] 
\centering
\includegraphics[width=0.5\textwidth]{dim_red}
\caption{Dimensionality Reduction Pipeline}
 \label{fig:Dimensionality Reduction}
 \end{figure}
\FloatBarrier
\bigskip

\subsection{Domain engineering Vs Unsupervised learning Vs Supervised learning}
\textit{Reduction by Domain engineering :} When the operators performing reduction \textit{do not} use any information from the data, but are rather derived from knowing the domain specific properties of the data, then the resulting reduction is said to be \textit{engineered}. Coming up with such operators is usually time-consuming and requires expert knowledge.\\
\\
\textit{Reduction by Unsupervised learning :} When the operators performing reduction are computed by exploiting the representation \textit{structure} of the data, then the resulting reductions are said to be \textit{learned} from the data. When this learning problem \textit{does not} require any labelled data, then the reduction is said to be \textit{unsupervised}.\\
\\
\textit{Reduction by Supervised learning :} When the operators performing reduction computed by exploiting the information from labelled data, then the resulting reduction is said to be obtained by \textit{supervised learning} 

\subsection{Principal Component Analysis (PCA)}
\label{pca}

The representation $\textbf{X}$ is changed to a \textit{basis} that are functions of variance between the frequencies. This is done by computing the co-variance matrix $\bm{\Sigma}$ from the data and performing \textit{orthogonal decomposition} to compute it's basis. The co-ordinates of the resulting basis system are called \textit{principal components}. The key idea for reduction is to discard the information corresponding to \textit{low variance} basis. The computation of PCA reduction operator $\textbf{W}_{PCA}$ is elaborated below,  

\begin{enumerate}[label=(\alph*)]
\item Usually, representations of large samples (say $S$ samples) from the dataset ($ \textbf{S} = [\textbf{X}_{1}, \textbf{X}_{2}, ..., \textbf{X}_{S}])$ are used to compute the co-variance matrix. The columns of $\textbf{S}$ are centred by their mean and the covariance matrix is computed as,
\[
   \bm{\Sigma} = \textbf{E}[(\textbf{S} - \textbf{E}[\textbf{S}])(\textbf{S} - \textbf{E}[\textbf{S}])^{T}] = \frac{1}{\displaystyle\sum_{s}{P_{s}}}\textbf{\^S}\textbf{\^S}^{T} \quad \in \mathbb{S}^{R \times R}
\]
The covariance matrix $\bm{\Sigma}$ is symmetric positive definite and hence belongs to space of symmetric operators $\mathbb{S}$.
\item The eigen values and eigen vectors of $\bm{\Sigma}$ are computed. At this point, we use the Orthogonal Eigenvector Decomposition Theorem and infer that eigen vectors of symmetric matrix ($\bm{\Sigma}$) form an orthogonal basis in $\mathbb{R}^{R}$. 
\[
\bm{\Sigma} = \textbf{V}\bm{\Lambda}\textbf{V}^{T} \qquad \textbf{V} \in \mathbb{O}^{R \times R}, \quad \bm{\Lambda} \in \mathbb{D}^{R \times R}
\]
The columns of matrix $\textbf{V}$ form the basis. Since the columns are orthogonal to each other, $\textbf{V}$ belongs to a space of orthogonal operators $\mathbb{O}$. $\bm{\Lambda}$ is a diagonal matrix of eigen values.

\item The eigen values represent the magnitude of variance for each frequency dimension.  Hence, eigenvectors corresponding to large eigen values gives the coordinates corresponding to greater variance. So eigen vectors corresponding to top $T$ eigen values are retained, while ignoring coordinates of lower variance. The resulting change of coordinates matrix is then $\textbf{\^V} \in \mathbb{O}^{R \times T}$
   
\item Since $\textbf{\^V}$ is orthogonal, $\textbf{\^{V}}^{-1} = \textbf{\^{V}}^{T}$, and the resulting reduction for \textit{each sample} can computed as $\textbf{Y} = \textbf{\^{V}}^{T}\textbf{X}$, where $\textbf{X} \in \mathbb{R}^{R \times P}$, $\textbf{Y} \in \mathbb{R}^{T \times P}$ and $T < R$. Thus the reduction operator is
\[
\textbf{W}_{PCA} = \textbf{\^V}^{T}
\]

\end{enumerate}   

\begin{algorithm}
  \caption{$\textbf{W}_{PCA}$ = PCA($\textbf{X}_{1}, \textbf{X}_{2},..,\textbf{X}_{S}$)}\label{alg:pca}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{S} = [\textbf{X}_{1},..\textbf{X}_{S}] \qquad \textbf{X}_{s} \in \mathbb{R}^{R \times P_{s}}, \textbf{S} \in \mathbb{R}^{R \times Q},\qquad  Q = \displaystyle\sum_{s}{P_{s}}$
    \Statex \textbf{Output :} $\textbf{W}_{PCA} \in \mathbb{R}^{T \times R}$
      \State $\bm{\Sigma} = \frac{1}{Q}\textbf{S}\textbf{S}^{T}$ \Comment{$\bm{\Sigma} \in \mathbb{S}^{R \times R}$}
      \State $\textbf{V}^{T} \bm{\Lambda} \textbf{V} = \bm{\Sigma}$ \Comment{$\bm{\Lambda} \in \mathbb{D}^{R \times R}$ , $\textbf{V} \in \mathbb{O}^{R \times R}$}
      \State $\textbf{V} \leftarrow \textbf{V}[:][:T]$ \Comment{$\textbf{V} \in \mathbb{O}^{R \times T}$}
      \State $\textbf{W}_{PCA} = \textbf{V}^{T}$
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\noindent Since the operator $\textbf{W}_{PCA}$ is computed from the data without requiring labelling, this method falls under \textit{unsupervised learning}. The columns of representation are first centred before applying the PCA reduction operation. An illustration of PCA reduction function $D_{(PCA)}$ is shown below,

\begin{algorithm}
  \caption{$\textbf{Y}$ = $D_{(PCA)}$($\textbf{X}$)}\label{alg:dpca}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{X} \in \mathbb{R}^{R \times P}$
    \Statex \textbf{Output :} $\textbf{Y} \in \mathbb{R}^{T \times P}$
	\State $\textbf{\^X} = \textbf{X} - \textbf{E}[\textbf{X}]$
	\State $\textbf{Y} = \textbf{W}_{PCA}\textbf{\^X}$
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\noindent Sometimes, to make the resulting reduction $\textbf{Y}$ uncorrelated (that is, unit variance in each frequency dimension), the corresponding dimensions are divided by their eigen values. This is because eigen values are proportional to the magnitude of variance in each frequency direction. This operation is known as \textbf{PCA Whitening} and the reduction operator is,
\[
\textbf{W}_{PCAW} = \bm{\Lambda}^{-1}\textbf{V}^{T}
\]    
The reduction function $D_{(PCAW)}$ is same as algorithm \ref{alg:dpca}, except that the operator $\textbf{W}_{PCAW}$ is used instead of $\textbf{W}_{PCA}$
\bigskip

\subsection{Mel-frequency cepstrum coefficients (MFCC)}
\label{mfcc}
It has been studied that the basis functions resulting from co-variance matrix of log-mel spectrogram representation are similar to cosine transform on log-mel spectrogram []. Therefore, instead of explicitly computing the basis functions from the data, one can simple use cosine basis. The co-ordinates of corresponding basis system are known as \textit{Mel-Frequency Cepstrum Coefficients}. The co-ordinates of high frequency cosine functions are discarded because they correspond to low-variance information. 
\bigskip

\noindent Since this reduction is engineered only for log mel spectrogram representation, the reduction function $D_{(MFCC)}$ takes the signal as input. $\textbf{W}_{MFCC}$ is the reduction operator.

\begin{algorithm}
  \caption{$\textbf{Y}$ = $D_{(MFCC)}$($\textbf{a}$) }\label{MFCC}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $\textbf{Y} \in \mathbb{R}^{T \times P}$
    \State $\textbf{X} = R_{(MEL)} \big( \textbf{a} \big) $ \Comment{$\textbf{X} \in \mathbb{R}^{R \times P} $} 
    \State $\textbf{X} \leftarrow ln(\textbf{X})$
    \For{$ \omega \in \{1,..,T\}$}
    \State $\textbf{W}_{MFCC}[ \omega ] \leftarrow \textbf{cos}( \omega \textbf{t})$  \Comment{$\textbf{W}_{MFCC} \in \mathbb{R}^{R \times T}, \textbf{t} \in \mathbb{R}^{R}$}
    \EndFor
    \State $\textbf{Y} \leftarrow \textbf{W}_{MFCC}\textbf{X}$
  \end{algorithmic}
\end{algorithm}
 

\subsection{Convolution neural network}
\label{stacked}
Transformation of input $\textbf{a}$ by shifted contractions of an operator $\textbf{w}$ is termed as \textit{discrete convolution} and can be mathematically represented as one dimensional convolution operation, 
\[
	\textbf{y} = \textbf{a} \star \textbf{w}^{(s)}
\]   
The operator $\textbf{w}$ is known as a \textit{filter function} and the length of shift $s$ is known as \textit{stride}. Usually $\textbf{a}$ is convolved with multiple filter functions. For $K$ \textit{filters},
\[
	\textbf{Y}[k] = \textbf{a} \star {\textbf{w}_{k}}^{(s)} = \textbf{a} \star \textbf{W}^{(s)} \qquad k \in {0,1..K-1}
\] 
The index based notations for this operation are shown in appendix ??. If the filters $\textbf{w}_{k}$ are \textit{defined}, then computation of $\textbf{Y}$ is straight forward. All the operations explained in the previous section are forward convolutions with defined filters,
\begin{itemize}
\setlength\itemsep{0em}

\item \textbf{STFT} in equation \ref{eq:stft}, where the filter functions are complex negative exponentials.
\item Transformation to \textbf{Mel} frequency scale in equation \ref{eq:mel}, where a set of mel-frequency centred filters are used.
\item \textbf{Principal components} and \textbf{MFCC} can be realized as a convolution of the input with transformation matrix $\textbf{V}^{T}$ defined in section \ref{basis} (Recall that matrix multiplication can be represented as a convolution of stride equal to column length. An illustration was shown in section \ref{mel})  

\end{itemize}
However, it is not clear if such defined filters really encodes the information in $\textbf{Y}$ relevant for certain classification task at hand. But, when a set of task-specific observations are available, it is possible to solve for the filters $\textbf{w}_{k}$, so that the resulting transformation $\textbf{Y}$ is optimal for the considered task. This is done using iterative optimization techniques, starting by random initializing of $\textbf{w}_{k}$ and updating it's values after every iteration. Computational models that solves by \textbf{first-order gradient descent} methods (a class of optimization techniques) are represented as first order \textbf{artificial neural networks}. The filters $\textbf{w}_{k}$ which we are attempting to solve are also termed as \textit{parameters} or \textit{weights} of the network. The iterative steps involving finding these \textit{weights} is termed as \textit{training} the neural network (Details of training neural network are explained in section \ref{training}). Since the transformation operation by the \textit{filters} are represented as convolution over the input, the neural network is termed as \textbf{convolution neural network}.   

\subsubsection{Approximating MFCC with CNN}
\textit{Supervised} feature learning has an advantage over \textit{unsupervised} feature learning when we wish to find filters $\textbf{w}_{k}$ optimal for context based classification tasks. Feature learning with CNN fall under \textit{supervised} feature learning category.
To show that CNN can do atleast as good as MFCCs, an illustration is discussed by approximating MFCC with CNN. Setting up a CNN architecture requires defining the \textit{hyper parameters} of like number of filters, filter dimensions and stride of the filter. The domain knowledge of MFCC computation is used to set these hyper parameters.
\bigskip  

\noindent To compute MFCC features, the input signal $\textbf{a}$ is convolved with complex negative exponentials ($\textbf{W}_{STFT}$). After element-wise squaring operation, the resulting transformation is convolved with mel-filters ($\textbf{W}_{MEL}$). Logarithm of the resulting transformation is then convolved with cosine filters ($\textbf{W}_{MFCC}$). The motivation and description of these so called \textit{engineered} filters were described in section \ref{time} and \ref{dimension}. 

\begin{algorithm}
  \caption{$\textbf{Y}$ = MFCC($\textbf{a}$) }\label{MFCC_engineer}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $\textbf{Y} \in \mathbb{R}^{T \times Q}$ 
    \State $\textbf{C} = \textbf{a} \star {\textbf{W}_{STFT}}^{(s)}$ \Comment{$ \textbf{W}_{STFT} \in \mathbb{R}^{M \times F}, \textbf{C} \in \mathbb{C}^{M \times Q}$}
    \State $\textbf{C} \leftarrow \textbf{C} \odot \textbf{C}$ 
    \State $\textbf{X} = \textbf{C} \star {\textbf{W}_{MEL}}^{(M)}$ \Comment{$ \textbf{W}_{MEL} \in \mathbb{R}^{R \times M}, \textbf{X} \in \mathbb{R}^{R \times Q}$}
    \State $\textbf{X} \leftarrow ln(\textbf{X})$
    \State $\textbf{Y} = \textbf{X} \star {\textbf{W}_{MFCC}}^{(R)}$ \Comment{$\textbf{W}_{MFCC} \in \mathbb{R}^{T \times R}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier
\noindent An equivalent of MFCC computation can be realized with three layers of convolution by replacing the engineered filters by learnable filters and setting the following hyper parameters :
\begin{itemize}
\setlength\itemsep{0em}
\item $\textbf{W}_{L1}$ : $M$ filters (representing discrete frequencies) of size $F$ (STFT window size) and stride $s$ (STFT hop length)
\item $\textbf{W}_{L2}$ : $R$ filters (for mel-frequencies) of size $M$ and stride $M$.
\item $\textbf{W}_{L3}$ : $T$ filters (for mel coefficients) of size $R$ and stride $R$.   
\end{itemize}
The non-linearity in between each layer is needed. Otherwise, the filters accross layers can be combined into a representation for single layer. Setting $\bm{\Phi}_{1}$ as element-wise squaring operation and $\bm{\Phi}_{2}$ as logarithm should result in a CNN architecture that \textit{may} realize MFCCs. That is, the solution will converge to the defined filters ($\textbf{W}_{STFT}, \textbf{W}_{MEL}, \textbf{W}_{MFCC}$) if they are optimal for the task considered. Otherwise, one could expect either richer representation or sub-optimal representation because of local convergence.  
\begin{algorithm}
  \caption{$\textbf{Y}$ = CNN($\textbf{a}$) }\label{MFCC_learn}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{a} \in \mathbb{R}^{N}$
    \Statex \textbf{Output :} $\textbf{Y} \in \mathbb{R}^{T \times Q}$ 
    \State $\textbf{C} = \bm{\Phi}_{1} (\textbf{a} \star {\textbf{W}_{L1}}^{(s)})$ \Comment{$ \textbf{W}_{L1} \in \mathbb{R}^{M \times F}, \textbf{C} \in \mathbb{C}^{M \times Q}$}
    \State $\textbf{X} = \bm{\Phi}_{2} (\textbf{C} \star {\textbf{W}_{L2}}^{(M)})$ \Comment{$ \textbf{W}_{L2} \in \mathbb{R}^{R \times M}, \textbf{X} \in \mathbb{R}^{R \times Q}$}
    \State $\textbf{Y} = \bm{\Phi}_{3} (\textbf{X} \star {\textbf{W}_{L3}}^{(R)})$ \Comment{$\textbf{W}_{L3} \in \mathbb{R}^{T \times R}$}
  \end{algorithmic}
\end{algorithm}
\FloatBarrier
\noindent It is possible to with-hold the engineered filters at earlier levels and just introduce learning at later stages. For instance, it is sometimes optimal to compute the STFT and mel-filters and just perform convolutions over the mel-spectrogram\cite{EndToEnd}\cite{choi_cnn} (see Chapter 3, Sec. ??). Sometimes it is also useful to generalize the convolution as a 2 dimensional operation\cite{MusicMotive}.(see Chapter 3, Sec. ??). 2D convolution operation is shown in appendix ??. 

\subsubsection{CNN as a general purpose feature extractor}  
\label{general}
In music information retrieval, several task-specific features have been engineered. The MFCC features along with it's derivatives (the derivative is useful to encode the temporal evolution) are often used for genre and mood recognition tasks. Instead of convolving with mel-filters on STFT representation, one might opt for filters that would result in chroma-gram (combines frequencies by exploiting the periodicity of pitches) or tempo-gram (encodes change of frequencies over time). Features derived from chroma-gram find application for automatic mixing, chord recognition tasks. Features derived from tempo-gram find applications for tasks like on-set detection, tempo-estimation etc.. But more often, combination of features are used to boost classifier performance. For all these features that can be realized in terms of hierarchy of convolution operations, it is possible to find mathematical equivalence in convolution neural network.     

\begin{figure}[h] 
\centering
\includegraphics[width=0.95\textwidth]{dnn_motivation}
\caption{General purpose feature extractor}
 \label{fig:deep learning}
 \end{figure}
\FloatBarrier
\bigskip

\section{Temporal Approximation}
\label{temporal}
The size of resulting features computed through reduction operations discussed in the previous section depends on length of the audio. But classifiers like \textit{support vector machines} and \textit{multi layer perceptron} requires features of fixed size for all data to compute the parameters while training. Therefore, frame-wise features over time are approximated to a fixed size feature representation. Temporal approximation $T$ of the reduced representation $\textbf{Y}$ is  
\[
\textbf{f} = T(\textbf{Y}) \qquad \textbf{f} \in \mathbb{R}^{Z}, \textbf{Y} \in \mathbb{R}^{T \times W}
\]
$W$ depends on length of the audio. $\textbf{f}$ is usually the final fixed size feature that is given as input to the classifier.  
\bigskip

\noindent The transformation by approximation function $T$ is done by using one of the methods,
\begin{itemize}
\setlength\itemsep{0em}
\item \textit{unsupervised} methods like \textit{statistical pooling} or \textit{clustering techniques} (Bag Of Frames, Gaussian mixture models)
\item \textit{supervised} methods with \textit{recurrent neural network}. 
\end{itemize} 
 
\subsection{Bag Of Frames}
\label{clustering}
In Bag of Frames(BoF) model, the \textit{frequency} of each reduced frame is used as a feature for training a classifier. To reduce to a fixed size representation (of size $Z$), $Z$ cluster centres are computed from the data. Each frame-wise feature is assigned to it's nearest cluster. The number of assignments to each cluster now becomes the feature. 
\begin{algorithm}
  \caption{$\textbf{f}$ = BagOfFrames($\textbf{Y}$) }\label{bof}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{Y} \in \mathbb{R}^{T \times W}$
    \Statex \textbf{Output :} $\textbf{f} \in \mathbb{R}^{Z}$
    \State $\textbf{f} = 0$
    \For{$ i \in \{0,1,..,W-1\}$}
    \State $j = \argmin\limits_{j} \norm{\textbf{Y}[:,i] - \bm{\mu}_{j}}^{2}$ \Comment{$\bm{\mu}_{j} \in \mathbb{R}^{T}, j \in \{0,1..Z-1\}$}
    \State $\textbf{f}[j] \leftarrow \textbf{f}[j] + 1$
    \EndFor
  \end{algorithmic}
\end{algorithm}
\FloatBarrier
\noindent The cluster centres are computed by clustering algorithms likes gaussian mixture models or K-Means clustering. Computation of $\bm{\mu}_{j}$ by K-Means is illustrated in algorithm \ref{alg:kmeans}. The cluster centres are computed for the entire dataset. Each of the $N$ sample track could have $W$ frame-wise features. To compute these centres, $\bm{\mu}_{j}$ are first randomly initialized. Then each frame level feature is assigned to the nearest centre. After the assignment of all frame features, the cluster centres $\bm{\mu}_{j}$ are re-computed. The frame features are then re-assigned to these new centres. This re-assignment and re-computation of $\bm{\mu}_{j}$ is iterated until convergence.  
\begin{algorithm}
  \caption{K-MEANS($\textbf{Y}_{0}, \textbf{Y}_{2},..., \textbf{Y}_{N-1}$) }\label{alg:kmeans}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{Y}_{n} \in \mathbb{R}^{T \times W}, n \in \{0,1,...,N-1\}$
    \Statex \textbf{Output :} $\bm{\mu}_{j} \in \mathbb{R}^{T}, j \in \{0,1,..,Z-1\}$
    \State Randomly initialize $\bm{\mu}_{j}$
    \While{$\bm{\mu}_{j}$ have not converged}
    \For{$n \in \{0,1,..,N-1$}
      \For{$ i \in \{0,1,..,W-1\}$}
         \State $j = \argmin\limits_{j} \norm{\textbf{Y}_{n}[:,i] - \bm{\mu}_{j}}^{2}$
          \State Assign $\textbf{Y}_{n}[:,i]$ to cluster $j$
      \EndFor
    \EndFor
   \State Recompute cluster means $\bm{\mu}_{j}$
   \EndWhile
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\subsection{Recurrent Neural Networks}
\label{rnn}
The idea behind RNN is to learn a feature representation by sequentially combining the input into an internal state $\textbf{h}$. The resulting feature is a projection from this internal state.
\[ 
\textbf{f} = \textbf{W}\textbf{h}_{W} \qquad \textbf{W} \in \mathbb{R}^{Z \times K}, \textbf{h}_{W} \in \mathbb{R}^{K}
\]
\[
\textbf{h}_{W} = \bm{\Theta}(\textbf{Y}[:,W], \textbf{h}_{W-1}) \qquad \textbf{Y} \in \mathbb{R}^{T \times W}
\]
$\textbf{h}_{W}$ is the internal state after combining $W$ columns of $\textbf{Y}$. $\bm{\Theta}$ is the internal state function which sequentially combines the input. The operator $\textbf{W}$ and other operators resulting from the function $\bm{\Theta}$ are solved for optimality using a labelled dataset. These operators can be computed by training an RNN (see Section \ref{training}). The function $\bm{\Theta}$ should sufficiently hold the information from beginning to end of the sequence. RNNs face challenge in remembering the information in the earlier part of the sequence. Because of this, different flavours of RNN have been developed to hold the sequence information longer and to battle the vanishing gradient problem while training neural network (see Section \ref{training}). Depending on how $\bm{\Theta}$ is defined, at least two RNN architectures are popular in the literature: Long-Short Term Memory (LSTM) RNN and Gated Recurrent Unit(GRU) RNN. In this thesis, only LSTMs [] are considered.   

\subsubsection{Long-Short Term Memory RNN}
The LSTM internal state is contained by three gates that control the information flow. This is done by multiplying the output of each sequence $\textbf{o}_{w}$ with the corresponding cell state function $\textbf{c}_{w}$.
\[
\textbf{h}_{w} = \textbf{o}_{w} \odot \sigma_{h}(\textbf{c}_{w}) \qquad w \in \{1,2,...,W\}
\] 
$\sigma_{h}$ is hyperbolic tangent function which projects the values between -1 and 1. The output gate $\textbf{o}_{w}$ combines the $w^{th}$ vector in sequence with the previous internal state ($\textbf{h}_{w-1}$) and projects the result between 0 and 1 with sigmoid activation ($\sigma$). This indicates the contribution of current sequence $w$ to the cell state. 
\[
\textbf{o}_{w} = \sigma(\textbf{W}_{o}\textbf{Y}[:,w] + \textbf{U}_{o}\textbf{h}_{w-1})
\]
The cell state $\textbf{c}_{w}$ acts as a conveyor belt where the information can either flow unchanged or get modified with update ($\textbf{i}_{w}$) and forget functions ($\textbf{g}_{w}$).
\[
\textbf{c}_{w} = \textbf{g}_{w} \odot \textbf{c}_{w-1} + \textbf{i}_{w} \odot \sigma_{h}(\textbf{W}_{c}\textbf{Y}[:,w] + \textbf{U}_{c}\textbf{h}_{w-1})
\] 
The operators of forget gate $\textbf{W}_{g}$ and $\textbf{U}_{g}$ control the deletion of information from the\textit{ previous} sequence. The output of $g_{w}$ is between 0 (delete the information) and 1 (keep the information). 
\[
\textbf{g}_{w} = \sigma(\textbf{W}_{g}\textbf{Y}[:,w] + \textbf{U}_{g}\textbf{h}_{w-1})
\] 
The operators of update gate $\textbf{W}_{i}$ and $\textbf{U}_{i}$ control the addition of information from the \textit{current} sequence. The output of $i_{w}$ is also between 0 and 1.
\[
\textbf{i}_{w} = \sigma(\textbf{W}_{i}\textbf{Y}[:,w] + \textbf{U}_{i}\textbf{h}_{w-1})
\] 
The operators $\textbf{W}_{o}, \textbf{U}_{o}, \textbf{W}_{i}, \textbf{U}_{i}, \textbf{W}_{g}, \textbf{U}_{g}, \textbf{W}_{c}$ and $ \textbf{U}_{c}$ are solved by training the RNN.
\bigskip

\noindent To get a fixed size temporal approximation, the RNN should project all the input sequence to a single output. This architecture of RNN is called \textit{Sequence to One} RNN. 
\begin{algorithm}
  \caption{$\textbf{f}$ = $Seq2One\_LSTM$($\textbf{Y}$) }\label{alg:s2olstm}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{Y} \in \mathbb{R}^{T \times W}$
    \Statex \textbf{Output :} $\textbf{f} \in \mathbb{R}^{Z}$
    \State Initialize $\textbf{h}_{0}$
    \For{$ w \in \{1,2,..,W\}$}
    \State  $\textbf{h}_{w} \leftarrow \bm{\Theta}_{LSTM}(\textbf{Y}[:,w],\textbf{h}_{w-1})$
    \EndFor
    \State $\textbf{f} = \textbf{W}\textbf{h}_{W}$
  \end{algorithmic}
\end{algorithm}
\FloatBarrier

\noindent Often times, multiple layers of RNN are used. An illustration of  two layer LSTM with a \textit{sequence to sequence} LSTM in between is shown in algorithm \ref{alg:2lstm}. The input sequence transformed in to a hidden sequence, which is then projected to a single output by the second layer. The functions in between ($\Phi_{1}, \Phi_{2}$) represents the transition operation between the layers. Several non-linear activations and transition operations have been developed to address the problems while training deep neural network (see Sec. \ref{training}). 
  
\begin{minipage}[t]{7.5cm}
  \vspace{0pt}  
\begin{algorithm}[H]
  \caption{$\textbf{f}$ = $LSTM2(\textbf{Y})$}\label{alg:2lstm}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{Y} \in \mathbb{R}^{T \times W}$
    \Statex \textbf{Output :} $\textbf{f} \in \mathbb{R}^{Z}$
    \Statex
    \Statex
    \Statex
    \State $\textbf{F}_{1} = \Phi_{1}(Seq2Seq\_LSTM(\textbf{Y}))$
    \State $\textbf{f} = \Phi_{2}(Seq2One\_LSTM(\textbf{F}_{1})$
  \end{algorithmic}
\end{algorithm}
\end{minipage}%
\begin{minipage}[t]{7.5cm}
  \vspace{0pt}
\begin{algorithm}[H]
  \caption{$\textbf{F}$ = $Seq2Seq\_LSTM$($\textbf{Y}$) }\label{alg:s2slstm}
  \begin{algorithmic}[1]
    \Statex \textbf{Input :} $\textbf{Y} \in \mathbb{R}^{T \times W}$
    \Statex \textbf{Output :} $\textbf{F} \in \mathbb{R}^{Z \times W}$
    \State Initialize $\textbf{h}_{0}$
    \For{$ w \in \{1,2,..,W\}$}
    \State  $\textbf{h}_{w} \leftarrow \bm{\Theta}_{LSTM}(\textbf{Y}[:,w],\textbf{h}_{w-1})$
    \State $\textbf{F}[:,w] = \textbf{W}\textbf{h}_{W}$
    \EndFor
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\FloatBarrier
{\color{gray}
\section{Training}
\label{training}
}
