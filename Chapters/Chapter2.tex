% Chapter 2

\chapter{Fundamentals} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 


%----------------------------------------------------------------------------------------

%\section{Welcome and Thank You}
In this chapter, acoustical characteristics of music signal that enables general MIR tasks will be introduced.We will examine the Fourier Series representations of sound waves and see how they relate to harmonics and tonal color of instruments  



%----------------------------------------------------------------------------------------

\section{Representation of audio signal}
The traditional way of observing signals is to view them in the time domain. The time domain is a record of what happened to a parameter of the system versus time. Standard formats use change of amplitude with time. However, it is useful to change the representation to frequency domain of the signal, which is also called spectrum. This is simply because our ear-brain combination is an excellent frequency domain analyzer. The ear-brain splits the audio spectrum into many narrow bands and determines the power present in each band. Hence, it can easily pick small sounds out of loud background noise. [pp1]
 
It was shown over one hundred years ago by Baron Jean Baptiste Fourier that any waveform that exists in the real world can be generated by series of sinusoids which are a function of frequencies. Hence any stationary signal  (i.e signal at time ‘t’ ) can be represented as a function of a fundamental frequency (lowest frequency), and other frequencies which are multiples of fundamental. The following abstract representation is adapted for further explanations in this section

EQ


\subsection{ Discriminants of music signal - Harmonics and overtones }
When a note is played on an instrument, listeners hear the played tone as the fundamental, as well as a combination of its harmonics sounding at the same time (pitch) (Hammond, 2011). 
Harmonics are tones that have frequencies that are integer multiples of the fundamental frequency. The fundamental and its harmonics naturally sound good together.

EQ, ki = integers 

These additional frequencies determines the timber of the instrument. The strength, or amplitude, of each harmonic is the difference we’re hearing, since each note played includes the fundamental tone and some harmonics. The instruments timbre is what distinguishes its sound from that of a different instrument. 

(graph example of harmonics of two instruments)

The presence of multiple, simultaneous notes in polyphonic music renders accurate pitch tracking very difficult. However, there are many other applications, including chord recognition and music matching, that do not require explicit detection of pitches, and for these tasks several representations of the pitch and harmonic information commonly appear. Usually, there are more instruments being played simultaneously and sometimes accompanied by voices. In such cases, we hear the fundamental and overtones (chord). The overtones are any frequency above the fundamental frequency.  The overtones may or may not be harmonics. So overtones are those frequencies which are not just restricted to integer multiples of fundamental. The fundamental and overtones together are called partials

EQ, ki != integers

m1(t) = f(440 ,880) \\
   Where 440 = fundamental, 880 = first harmonic
m2(t) = f(330 , 660)\\
   Where 330 = fundamental, 660 = first harmonic
   
Thus the recorded signal has components of all these frequencies\\
  m(t) = f(330,440,660,880)\\
  Where 330 = fundamental, 440 = second partial, 660 = third partial, 880 = fourth partial
  
Most certainly, the signal evolves over time and hence the components of frequencies and its amplitudes will vary for each time t. The heat map representation of amplitudes, with frequency along y, time along   x is called spectrogram. 

Thus to discriminate a signal, we not only need the evolution of frequencies, but also information about harmonics. For instance, to discriminate the instruments from the recorded signal m(t), the classifier should infer the frequencies in the each harmonics m1(t) and m2(t). To discriminate the temporal pattern (Rhythm), we need the evolution of m1 and m2. To identify other aesthetics (warm, city) , the interaction between m1(t) and m2(t) should be inferred. To discriminate voices and other non-harmonic aspects (tempo, beat), the envelop curve of the spectrum will also be needed.    

(diagram)


\subsection{Sampling of continuous-time signal}
The digital formats contain the discrete version of the signal obtained by sampling continuous-time signal. For functions that vary with time, let s(t) be a continuous function (or "signal") to be sampled, and let sampling be performed by measuring the value of the continuous function every T seconds, which is called the sampling interval or the sampling period.[1][pp2]. The sampling frequency or sampling rate, fs, is the average number of samples obtained in one second (samples per second),  

thus fs = 1/T.

The optimum sampling rate is given by Nyquist-Shannon sampling theorem which says, the sampling frequency (fs) should be at least twice the highest frequency contained in the signal [pp2] Given the human hearing range lies between 20Hz - 20KHz [pp3], most of the digital audio formats use a standard sampling frequency of 44.4Khz. The signal is further downsampled depending on the kind of feature information needed for classification. 







