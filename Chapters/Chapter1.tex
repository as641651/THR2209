% Chapter 1
\externaldocument{chapter3}
\externaldocument{chapter4}
\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

%\section{Welcome and Thank You}

Computers have been used to automate discovery and management of music in so many different ways. Automating the task of attaching a semantic meaning to a song is popularly known as \textit{music auto-tagging}. Automatic tagging algorithms have been used to build recommendation systems that allow listeners to discover songs that match their taste. It also enables online music stores to filter their target audience. But semantic description of a song is not straightforward and there is this gap between music audio and listener's description, both linguistically and emotionally, which we term as \textit{audio-semantic gap}. In this thesis, we test the state-of-art approaches of music auto-tagging on a dataset that is least affected by \textit{audio-semantic} noise. In section \ref{motivation}, the need for this dedicated research is explained by describing some shortcomings of the currently available solution approaches. In section \ref{overview}, a top to bottom overview of the contents of this research is presented.   

%----------------------------------------------------------------------------------------

\section{Motivation}
\label{motivation}
Captioning music from the point of view of an artist is an interesting application. Training an algorithm to do so takes music auto-tagging a step towards human intelligence. Although great technical progress have been made to enable efficient retrieval and organization of audio content, analysing music and communicating it's artistic properties is still challenging. Current music recommendation systems fall short in providing recommendations based of aesthetics of music because of the reasons described below,
  
\subsection{Cold start problem with collaborative filtering methods}
When the usage data is available, one can use collaborative filtering to recommend the tracks on a community-based trending lists (say, a community of experts). That is, if a listener liked songs A and B and you liked A, you might also like B. Such algorithms have proven to be extremely efficient and out-perform those algorithms that works by extracting acoustic cues from audio signal for the task of finding similar songs \cite{DC1}. However, in absence of such usage data, one resorts to content-based methods, where just the audio signal is used for generating recommendations. Thus collaborative filtering methods suffer from what is called a \textit{cold start problem}, making it less efficient for new and unpopular songs. 


\subsection{Problems with content based methods}
\label{problems}
Using information from audio content to overcome the cold-start problem resulted in \textit{content-based} recommendation methods. In such algorithms, a classifier is trained on some training data to learn acoustic cues. But a recommendation system can also be built without requiring  such acoustic labels by combining \textit{content based} and \textit{collaborative} techniques[combining content] and training it from a collaborative view point. However, this is not sufficient if a recommendation system has to designed for artists and composers who search for songs based on some properties of music itself. In such cases, the current \textit{content-based} methods fall short because of following training assumption,   

\subsubsection{Psychoacoustic assumptions}
Music descriptions are often affected by social factors. However, it is possible to measure what percentage of subjects classify a music to certain mood (say, happy, dull etc.) and present the popular description. The currently available large datasets are built on this assumption \cite{MSD}\cite{MTT}. Algorithms trained on such datasets may converge to learning social descriptions rather than actual descriptions termed by experts for the properties of music (also termed as \textit{aesthetics} in popular journals of music psychology [no account for taste..]). This psychoacoustic assumption stands as a barrier to discover songs based on aesthetics (which has applications in music therapy \cite{MusicTherapy}).

\subsubsection{Temporal summarization of audio content}
The current state of art music tagging algorithms\cite{choi_crnn}\cite{MultiScale} are established by training on datasets that contain just short music excerpts. In run-time, these algorithms classify each short section separately and merge tags across different sections. But an artist might describe the music as a whole and not in sections. Hence there is a need to test algorithms that extract features approximating greater length of the song. 

\subsection{Need for adaptive glossary}
Current recommendation systems including the ones that use collaborative filtering, restrict the user with the choice of tags. Moreover, it is not guaranteed that all users will perceive all the tags in the same way. A recent study in idiographic music psychology have indicated that different people use different aesthetic criteria to make judgement about music\cite{NoAccountingForTaste}. Hence it would be interesting to study if these models \cite{choi_cnn}\cite{choi_crnn} trained on large datasets can be exploited for training on personal repertoire, which are usually small.  

%----------------------------------------------------------------------------------------

\section{Overview}
\label{overview}
Convolution neural networks (CNN) have recently gained popularity for content-based multi-label classification task achieving state-of-art performance on established datasets\cite{choi_cnn}\cite{choi_rnn}. But these models were trained on large amount of training data containing short excerpts of music and it is not clear if section-wise merging of descriptions can approximate actual description of the whole song. To train these models to tag like an artist, a dataset tagged by an expert is needed and gathering such data is expensive. So the aim of this thesis is to find out
\begin{itemize}
\setlength\itemsep{0em}
\item If the celebrated CNN models trained on large data can be exploited to show similar performance gains when \textit{fine-tuned} on a small dataset (Generally termed as \textit{transfer learning}).
\item Models that can best approximate signals of arbitrary length to a fixed size representation (needed for classification).
\end{itemize} 
    
\noindent The classification is done on a lower dimensional approximation of the audio signal known as \textit{feature}. The general pipeline for music feature extraction is \textit{signal representation}, systematic \textit{dimensionality-reduction} followed by \textit{temporal approximation}.
  
\subsection{Signal Representation}
Music is distinguished by the presence of many relationships that can be treated mathematically, including rhythm and harmony, by analysing the frequency content. In Chapter 2 (Sec. \ref{time}), representation of digital audio in time-frequency format is explained. Motivated by the way ear-brain handles the frequency information, myriad of features extracted from spectrogram representations were evolved. Each one of them will fit into one of these broad categories:
\begin{itemize}
\setlength\itemsep{0em}
\item \textbf{Mel based spectrogram :} Exploiting the fact that our ear cannot distinguish adjacent frequencies (say we cannot differentiate 300 KHz and 310 KHz), the information pertaining to frequencies are binned according to a what to popularly known as \textit{mel-scale}. The features (MFCCs, etc). obtained from this representation are useful for any general purpose application like speech recognition, genre classification etc.    
\item \textbf{Chromagram :} Frequencies are binned by taking advantage of the periodic perception of \textit{pitch} (perceived frequency). Features (Tonnetz, etc) following from this representation find applications in music mixing, chord recognition etc. 
\item \textbf{Tempogram :} For applications like tempo estimation and onset detection, the representation should encode the \textit{change} of frequencies over time. This resulted in a set of features (Novelty curve, beat centroid etc) calculated from \textit{differential} spectrograms.  
\end{itemize} 
In this thesis, we only study \textit{mel-based spectrogram} representations. Chroma and tempo features can be obtained after binning to mel-scale (not necessarily), and in this sense mel-spectrogram can be viewed as a super set. But one should look at this categorization in terms of mathematical operation. Features resulting from \textit{mel-spectrogram} means \textit{explicit} operations involving periodic binning and temporal differentiation are not involved. I use the word \textit{explicit} because it will be shown later that tempo and chroma information can be \textit{implicitly} modelled by \textit{feature learning} with CNNs (see Chapter 2, \ref{stacked}).     
  
\subsection{Dimensionality reduction}
Features are usually obtained as a result of some dimensionality reduction operation on the spectrogram. In Chapter 2 (Sec. \ref{basis}), dimensionality reduction through \textit{basis transformation} is introduced in terms of convolution operation and the extraction of Mel-Frequency-Cepstral-Coefficients (MFCCs) is explained. Then in section \ref{stacked}, generalizing the MFCC computation into a learning problem by replacing the basis functions with learnable convolutional filters is discussed and eventually explaining the success of convolution neural networks. In Chapter 3, some of the successful algorithms reported for music-auto-tagging are discussed. Transfer-learning using CNN model in \cite{choi_cnn} which reports close to state-of-art performance is compared with MFCC features for our target task in Chapter 4 (Sec. \ref{experiments}) .
    
\subsection{Temporal approximation}
We are looking for an algorithm that will approximate features for songs of arbitrary length without losing much of the rhythmic information. In the current literature, this is handled using \textit{Bag of Words} approach which is explained in Chapter 2, section \ref{clustering}. But as the reported performance are for 29.1s song clips, their optimality for songs of greater length is questioned. So we test \textit{Recurrent neural network (Sequence to One LSTM)} which is more motivating to use for rhythmic information extraction (see \ref{rnn}). In Chapter 4 (Sec. \ref{experiments}), the performance of approximation by \textit{Bag of words} and \textit{Recurrent neural network} is reported.


%----------------------------------------------------------------------------------------

\section{Outline of the report}
In chapter 2, the fundamentals and formalism of notations are introduced. In chapter 3, a detailed overview of previous research, their shortcomings for the current problem along with justification for proposed models are discussed. In Chapter 4, details of the dataset and the experiment results of proposed models are discussed. In chapter 5, the inference from these experiments in understanding the development of algorithms for tagging music with aesthetic tags is explained and future directions are discussed. 






