% Chapter 1
\externaldocument{chapter3}
\externaldocument{chapter4}
\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

%\section{Welcome and Thank You}

Computers have been used to automate discovery and management of music in so many different ways. Automating the task of attaching a semantic meaning to a song is popularly known as \textit{music auto-tagging}. Automatic tagging algorithms have been used to build recommendation systems that allow listeners to discover songs that match their taste. But semantic description of a song is not straightforward and there is this gap between music audio and listener's description. In section \ref{motivation}, the need for this dedicated research is explained by describing some shortcomings of the currently available solution approaches. In section \ref{overview}, an overview of the contents of this research is presented.   

%----------------------------------------------------------------------------------------

\section{Motivation}
\label{motivation}
Developing an algorithm that imitates the human way of describing auditory scene is an interesting application. Although great technical progress have been made to enable efficient retrieval and organization of audio content, analysing music and communicating it's musical syntax that respects multidimensional qualities of sound is still challenging. Current music recommendation systems fall short in providing recommendations by respecting the criteria emerging from perceptual qualities of music because of the reasons described below,
  
\subsection{Challenges with collaborative filtering methods}
When the usage data is available, one can use collaborative filtering to recommend the tracks on a community-based trending lists (say, a community of experts). That is, if a listener liked songs A and B and you liked A, you might also like B. Such algorithms have proven to be extremely efficient and out-perform those algorithms that works by extracting acoustic cues from audio signal for the task of finding similar songs \cite{DC1}. However, in absence of such usage data, one resorts to content-based methods, where just the audio signal is used for generating recommendations. Thus collaborative filtering methods suffer from what is called a \textit{cold start problem}, making it less efficient for new and unpopular songs. 


\subsection{Challenges with content based methods}
\label{problems}
Using information from audio content to overcome the cold-start problem resulted in \textit{content-based} recommendation methods. In such algorithms, acoustic cues required for discriminating the semantics are extracted from the audio signal. These acoustic cues have to be encoded in the lower dimensional approximation of audio signal, also known as features. However, to provide descriptions that could come closer to human intelligence, the features should also encode the perceptual information in the song. The current state-of-art content based music tagging algorithms\cite{MultiScale}\cite{choi_crnn} use large sets of data to train an algorithm to automatically detect the optimal features from a spectrogram representation of the audio signal. This kind of training is known as \textit{feature learning}. Unsupervised learning (training data is not labelled) used in \cite{MultiScale} may not extract features that would be optimal for all contexts of music tagging. Supervised learning used in \cite{choi_crnn}\cite{choi_cnn} can extract features optimal for context used, but need large amount of labelled data for training.      


%----------------------------------------------------------------------------------------

\section{Overview}
\label{overview}
Convolution neural networks (CNN) have recently gained popularity for content-based multi-label classification task achieving state-of-art performance\cite{choi_cnn}\cite{choi_crnn} on established datasets\cite{MSD}\cite{MTT}. But these models were trained on large amount of labelled training data containing short excerpts of music of fixed size (29.1s). The aim of this thesis is to find out
\begin{itemize}
\setlength\itemsep{0em}
\item If the CNN models trained on large data can be exploited to show similar performance gains with \textit{transfer-learning} on smaller dataset. That is, the CNN is first trained on a large labelled dataset and the converged weights are used as initialization for training on smaller dataset. 
\item Models that can extract features by approximating signals of arbitrary length.
\end{itemize} 
    
\noindent For the purpose of analysing feature learning, the feature extraction pipeline for music information retrieval is formalised into following stages: \textit{signal representation}, systematic \textit{dimensionality-reduction} followed by \textit{temporal approximation}.
  
\subsection{Signal Representation}
Music is distinguished by the presence of many relationships that can be treated mathematically by analysing the frequency content. Motivated by the way ear-brain handles the frequency information, myriad of features extracted from spectrogram representations were evolved. In this thesis, following the analyses from current literature \cite{choi_cnn}\cite{EndToEnd}\cite{MultiScale}\cite{choi_crnn}, only \textit{mel-spectrogram} representation is used. \textit{Mel-spectrograms} exploit the fact that our ear cannot distinguish adjacent frequencies (say we cannot differentiate 300 KHz and 310 KHz), and the frequencies values are binned according to a what to popularly known as \textit{mel-scale}. In chapter 2, section \ref{time}, representation of digital audio in time-frequency format is explained. In section \ref{stft}, computations of mel-spectrogram as a convolution operation is elaborated.
   
\subsection{Dimensionality reduction}
The signal representation is divided into short frames and the dimension of each frame is reduced systematically by discarding information that does not contribute to discrimination. The mathematical operators that discard information not contributing to variance in our context specific classification task can be obtained by solving for parameters of CNNs. In chapter 2, section \ref{stacked}, formalism of CNN as feature extractor for MIR tasks have been illustrated and in section \ref{training}, challenges in training CNN have been pointed out. The training dataset for our thesis (target dataset) is small. Therefore CNN is first trained on the publicly available \textit{MagnaTagATune} dataset\cite{MTT} and \textit{transfer-learning} on our target dataset is comparatively analysed in chapter 4, section \ref{cnn_s}.

\subsection{Temporal approximation}
The reduced information from each frame are temporally approximated to a fixed sized representation. The classifier then takes these fixed sized features and outputs the tags. The resulting approximation should not lose the perceptual information required for discrimination. To do this, supervised learning with \textit{sequence to one} Recurrent Neural Network (RNN) is used, that sequentially combines reductions from each frame to a fixed size feature. Formalisms of RNN are presented in chapter 2, section \ref{rnn} and comparative analyses of transfer learning with CNN + RNN models are discussed in chapter 4, section \ref{cnn_s} 
  
%----------------------------------------------------------------------------------------

\section{Outline of the report}
In chapter 2, the fundamentals of mathematical concepts used in this thesis are explained and formalism of notations are introduced. In chapter 3, a detailed overview of previous research and justification for our proposed models for analyses are discussed. In chapter 4, details of the datasets, evaluation metrics, parameter settings and the experiment results are analysed. In chapter 5, the inference from these experiments and future directions are proposed. 






